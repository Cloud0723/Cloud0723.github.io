<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Offline Inverse Reinforcement Learning</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">When Demonstrations Meet Generative World Models:
              A Maximum Likelihood Framework for Offline
              Inverse Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://siliangzeng.github.io/" target="_blank">Siliang Zeng</a><sup>*&loz;</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=DMLi140AAAAJ&hl=en" target="_blank">Chenliang Li</a><sup>*&ordm;</sup>,</span>
                  <span class="author-block">
                    <a href="https://engineering.tamu.edu/industrial/profiles/garcia-alfredo.html" target="_blank">Alfredo Garcia</a><sup>&ordm;</sup>,</span>
                    <span class="author-block">
                      <a href="https://people.ece.umn.edu/~mhong/mingyi.html" target="_blank">Mingyi Hong</a><sup>&loz;</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">NeurIPS 2023 Oral</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution<br>
                      <sup>&loz;</sup>Department of Electrical and Computer Engineering,<br>
                      University of Minnesota, MN, USA<br>
                      <sup>&ordm;</sup>Department of Industrial and Systems Engineering,<br>
                      Texas A&M University, TX, USA
                    </small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2302.07457.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/2302.07457.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Cloud0723/Offline-MLIRL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2302.07457" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <b>Overview:</b> We introduce the offline inverse reinforcement learning (<span class="dnerf">OFFLINE-MLIRL</span>) with provable associated statistical and computationa guarantees and outstanding empirical performances.
      </h2>
      <img src="./static/images/Overview.png" width="100%">
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Offline inverse reinforcement learning (Offline IRL) aims to recover the structure of rewards
and environment dynamics that underlie observed actions in a fixed, finite set of demonstrations
from an expert agent. Accurate models of expertise in executing a task has applications in
safety-sensitive applications such as clinical decision making and autonomous driving. However,
the structure of an expert’s preferences implicit in observed actions is closely linked to the
expert’s model of the environment dynamics (i.e. the “world”). Thus, inaccurate models of the
world obtained from finite data with limited coverage could compound inaccuracy in estimated
rewards. To address this issue, we propose a bi-level optimization formulation of the estimation
task wherein the upper level is likelihood maximization based upon a conservative model of
the expert’s policy (lower level). The policy model is conservative in that it maximizes reward
subject to a penalty that is increasing in the uncertainty of the estimated model of the world. We
propose a new algorithmic framework to solve the bi-level optimization problem formulation and
provide statistical and computational guarantees of performance for the associated optimal reward
estimator. Finally, we demonstrate that the proposed algorithm outperforms the state-of-the-art
offline IRL and imitation learning benchmarks by a large margin, over the continuous control
tasks in MuJoCo and different datasets in the D4RL benchmark.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Method</h2>
      <img src="./static/images/Pipeline.png" width="100%">
    </div>
  </div>
</section>


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiment Result</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/Mujoco_Result1k.png" alt="MY ALT TEXT"/>
        
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Mujoco_Result.png" alt="MY ALT TEXT"/>
        
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Mujoco_Result10k.png" alt="MY ALT TEXT"/>
        
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Reward_Correlation.png" alt="MY ALT TEXT"/>
        
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/Reward_transfer.png" alt="MY ALT TEXT"/>
        
     </div>
     
  </div>
</div>
</div>
</section>
<!-- End image carousel -->






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zeng2023understanding,
        title={Understanding Expertise through Demonstrations: A Maximum Likelihood Framework for Offline Inverse Reinforcement Learning},
        author={Zeng, Siliang and Li, Chenliang and Garcia, Alfredo and Hong, Mingyi},
        journal={arXiv preprint arXiv:2302.07457},
        year={2023}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
